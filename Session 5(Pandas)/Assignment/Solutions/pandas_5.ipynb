{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3 as sql\n",
    "sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sql.connect('../data/test.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1897)\n",
    "df_1 = pd.DataFrame({'Id':np.arange(1,21),\n",
    "                     'ProdId': ['A','B','C','D']*5,\n",
    "                     'Length': np.random.randint(20,40,size=20),\n",
    "                     'Height': np.random.randint(10,35,size=20)})\n",
    "df_2 = pd.DataFrame({'Id':np.random.randint(10,size=100),\n",
    "                     'Date':pd.to_timedelta(np.random.randint(0,1000,size=100), unit='d')+pd.datetime(2015,1,1),\n",
    "                     'Cat_1': np.array(['A','B','C','D'])[np.random.randint(4,size=100)],\n",
    "                     'Cat_2': np.array(['S','T','Q','R'])[np.random.randint(4,size=100)]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5.1\n",
    "Load the sheets `Customers` and `Products` from the Excel file `data/master_data.xlsx`. Store the data from the `Customers` sheet in `cust_df` and the data from the `Products` sheet in `prod_df`.\n",
    "<details><summary>Hint 1</summary>\n",
    "    <p>\n",
    "        The pandas function `read_excel` has an option `sheet_name`, that let's one specify which sheets to extract\n",
    "    </p>\n",
    "</details>\n",
    "<details><summary>Hint 2</summary>\n",
    "    <p>\n",
    "        The pandas function `read_excel` returns a dictionary with the sheet names as keys\n",
    "    </p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.read_excel('../data/master_data.xlsx',sheet_name=['Customers','Products'])\n",
    "cust_df = tmp['Customers']\n",
    "prod_df = tmp['Products']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5.2\n",
    "The CSV file `data/bad_form.csv` follows a rather odd format:\n",
    "* The columns are separated by `|`\n",
    "* There are several comment lines. Each comment is indicated by `//`\n",
    "* True values in the Boolean columns are marked by `Yes`\n",
    "* False values in the Boolean columns are marked by `Nope`\n",
    "* Decimals are indicated by `,`\n",
    "* The string columns contain characters in `latin-1` encoding\n",
    "* In column **B** the missing values are indicated by `Missing`\n",
    "* In column **C** the missing values are indicated by `Not here`\n",
    "* The first 5 lines in the file is merely a header - should not be loaded\n",
    "* The last 10 lines make out the footer - should not be loaded\n",
    "\n",
    "Load the file in a DataFrame with the column `Id` as index.\n",
    "<details><summary>Hint 1</summary>\n",
    "    <p>\n",
    "        Look into the documentation for parameters `sep`, `comment`, `true_values`, `false_values`, `index_col`, `skiprows`, `skipfooter`, `na_values`, `encoding` and `decimal` for the pandas function `read_csv`.\n",
    "    </p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('../data/bad_form.csv',\n",
    "                 sep='|',\n",
    "                 engine='python',\n",
    "                 comment='//',\n",
    "                 true_values=['Yes'],\n",
    "                 false_values=['Nope'],\n",
    "                 index_col='Id',\n",
    "                 skiprows=5,\n",
    "                 skipfooter=10,\n",
    "                 na_values={'B':'Missing','C':'Not here'},\n",
    "                 encoding='latin-1',\n",
    "                 decimal=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5.3\n",
    "Write the content of the DataFrame `df_1` into the table `product_dimensions` in the database `test.db`. The connection to the database has already been set up in the variable `conn`.\n",
    "<details><summary>Hint 1</summary>\n",
    "    <p>\n",
    "        Use the `to_sql` method for the DataFrame to write it to the database\n",
    "    </p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.to_sql('product_dimensions', conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5.4\n",
    "Compute the average area of each product in `product_dimensions` using a SQL statement. Then proceed to load the data in two lines at a time, and print out the result.\n",
    "<details><summary>Hint 1</summary>\n",
    "    <p>\n",
    "        The pandas function `read_sql` can be used to parse a SQL statement to a database and store the result into a DataFrame\n",
    "    </p>\n",
    "</details>\n",
    "<details><summary>Hint 2</summary>\n",
    "    <p>\n",
    "        Setting the parameter `chunksize` controls the number of records returned at a time\n",
    "    </p>\n",
    "</details>\n",
    "<details><summary>Hint 3</summary>\n",
    "    <p>\n",
    "        With a set `chunksize` you can subsequently loop over the reader object, that is returned from `read_sql`\n",
    "    </p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = pd.read_sql('select ProdId, avg(Length*Height) from product_dimensions group by ProdId', conn, chunksize=2)\n",
    "\n",
    "for chunk in reader:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5.5\n",
    "For each `ID` extract the records up until the first time a `B` or a `C` appears in the `Cat_1` column, or a `R` or a `S` appears in the `Cat_2` column. The `Date` column indicates the order of the observations.\n",
    "<details><summary>Hint 1</summary>\n",
    "    <p>\n",
    "        The `groupby` method preserves the order of the records as they had in the original DataFrame\n",
    "    </p>\n",
    "</details>\n",
    "<details><summary>Hint 2</summary>\n",
    "    <p>\n",
    "        Applying the `cummax` method to a Boolean column returns 0 for every record, until the first True value is reached. From that point on it returns a 1.\n",
    "    </p>\n",
    "</details>\n",
    "<details><summary>Hint 3</summary>\n",
    "    <p>\n",
    "        Using the index to merge the results onto the original DataFrame makes it possible to select the rows with value 0 in the new column.\n",
    "    </p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df_2.merge(df_2\n",
    "                 .assign(event=((df_2['Cat_1'].isin(['B', 'C']))|\n",
    "                                (df_2['Cat_2'].isin(['R', 'S']))))\n",
    "                 .sort_values(['Date'])\n",
    "                 .groupby('Id')['event']\n",
    "                 .cummax()\n",
    "                 .to_frame(),\n",
    "                 right_index=True,\n",
    "                 left_index=True).sort_values(['Id','Date'])\n",
    "tmp[tmp['event']==0].drop('event', axis=1)"
   ]
  }
 ]
}